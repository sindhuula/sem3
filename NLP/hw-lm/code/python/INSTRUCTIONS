This Python port was kindly provided by Eric Perlman, a previous student in the
NLP class.  Thanks to Prof. Jason Baldridge at U. of Texas for updating the
code and these instructions when he borrowed the assignment.  They were
subsequently modified by Xuchen Yao and Mozhi Zhang for more recent versions of
the assignment.

----------------------------------------------------------------------

Python equivalent of the provided Perl code for assignment 2.

This code is an almost a literal translation, line for line,
of the perl code.  Almost no optimizations were done.  It runs
about 20% slower on my PowerBook, but uses 20% less memory, than
the Perl version.

Running this under more recent versions of Python will produce a
deprecation message regarding the xreadlines module.
# TODO for TA: is this still true?

For speed, you could try using a Python compiler such as PyPy (or Psyco).  

----------------------------------------------------------------------


Copy these code files to a private directory of yours, DIR.  
You can do this as follows:

  mkdir MYDIR
  cp -p /usr/local/data/cs465/hw2/code/python/* MYDIR
  cd MYDIR

----------

QUESTION 1.
Type "./fileprob.py" without arguments to see documentation.  
Try the examples mentioned in the documentation.

----------

QUESTION 2.

Copy fileprob.py to textcat.py.

Modify textcat.py so that it does text categorization.

For each training corpus, you should create a new language model.  You
will first need to call set_vocab_size() on the pair of corpora, so
that both models use the same vocabulary (derived from the union of
the two corpora).  Note that set_vocab_size can take multiple files as
arguments.  You can re-use the current LanguageModel object by using the
following strategy.

    call Probs.set_vocab_size() on the pair of training corpora

    train model from corpus 1
    for each file, 
       compute its probability under model 1; save this in an array

    re-train model from corpus 2
    for each file, 
       compute its probability under model 2; save this in an array

    finally, loop over the arrays to print the results    

----------

QUESTION 5.

Modify the prob() function in Probs.py.  You are just filling in the
case BACKOFF_ADDL.

Remember you need to handle OOV words, and make sure the probabilities
of all possible words after a given context sums up to one.

As you are only adding a new model, the behavior of your old models such
as ADDL should not change.

----------------------------------------------------------------------

QUESTION 6.

(a) Modify the prob() function in Probs.py.  You are just filling in the
case LOGLINEAR.

Remember you need to handle *OOL* words.  This is slightly different than
handling OOV words.

(b) Implement stochastic gradient ascent in the train() function in Probs.py.

(e) Modify both prob() and train() to add the new feature.

As you are only adding a new model, nothing that you do should change
your previous results.

Using vector/matrix operation (optional):
Training the log-linear model on en.1K can be done with simple "for" loops and
2D array representation of matrices.  However, you're welcome to use NumPy's
matrix/vector operations, which might reduce training time and simplify your
code.

----------

QUESTION 8.
Copy fileprob to speechrec.
Modify speechrec so that it does text categorization.
You only have one training corpus now, making things easier.

----------

Extra Credit Question 10.  

Modify the prob() function in Probs.py.  You will fill in the case
BACKOFF_WB.

You'll also need to modify other parts of Probs.pm -- in particular,
adding code to the train() subroutine and the global variable
declarations.

Again, as you are only adding a new model, nothing that you do should
change your previous results.

----------
