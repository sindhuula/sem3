This Java port of the Python code was kindly provided by Elias Ponvert
<ponvert@mail.utexas.edu>, after a class at U. Texas borrowed an earlier
version of this assignment.  The code and instructions were subsequently 
modified by Xuchen Yao, Mozhi Zhang and Dingquan Wang for more recent versions of the
assignment.

----------------------------------------------------------------------

Copy these code files to a private directory of yours, DIR,
and compile them there.  You can do this as follows:

  mkdir MYDIR
  cp -p /usr/local/data/cs465/hw2/code/java/* MYDIR
  cd MYDIR

THE FILES YOU ARE RESPONSIBLE FOR ARE:

  - BackoffAddLambdaLanguageModel.java
  - LoglinearLanguageModel.java
  - a new file TextCat.java (based on Fileprob.java)
  - a new file SpeechRec.java (based on Fileprob.java)
  Extra Credit:
  - BackoffWittenBellLanguageModel.java

You should not need to modify any of the other sample files, and if
you don't, it will suffice to submit only these files for the
homework. Of course, if you modify any of the other files in the code
sample, please include those when you submit your solution.

----------------------------------------------------------------------

QUESTION 1.  
To compile the classes and the sample FileProb program:
  javac *.java

Now you can run the fileprob command.  Run it without arguments to see
documentation:
  java FileProb
Try the examples mentioned in the documentation.

Warning: If your machine doesn't have enough memory, it will swap to
disk and take forever.  To see how many megabytes of memory a Unix
machine has, use "free -m".  The "top" command lets you monitor the
size of running processes and the amount of time that the machine is
waiting for the disk (iowait).

I think you should be okay on the ugrad machines (especially ugradx).  
Note that Witten-Bell may increase your memory requirements.  The
problem is most likely to arise on the "switchboard" corpus, which
is largest.

----------------------------------------------------------------------

QUESTION 2.  
Create a new class TextCat.java with a static void main(String[] args)
method. Optionally, you may copy FileProb to TextCat and update the
class name, or call FileProb.fileLogProb from a new TextCat class --
you'll need this method in TextCat either way.

For each training corpus, TextCat should create a new AddLambdaLanguageModel.
You will first need to call setVocabSize on the pair of corpora, so
that both models use the same vocabulary (derived from the union of
the two corpora).  Note that setVocabSize can take multiple files as
arguments.

One general solution to this problem may be of the form:

  call lm.setVocabSize() on the pair of training corpora

  train model from corpus 1
  for each file,
    compute its probability under model 1: save this in an array

  train model from corpus 2
  for each file,
    compute its probability under model 2: save this in an array

  finally, loop over the arrays and print the results

There are other ways to solve this problem, if you prefer.

----------------------------------------------------------------------

QUESTION 5.
Modify BackoffAddLambdaLanguageModel.java to complete this problem. In
particular, complete the implementation of the prob() method. 

As you are only adding a new model, the behavior of the old models
such as ADDL should not change.

----------------------------------------------------------------------

QUESTION 6.
Modify LoglinearLanguageModel.java to complete this problem.  In particular,
you need to finish the train() and prob() methods.

Remember you need to handle *OOL* words.  This is slightly different than
handling OOV words.

For part (e), modify both prob() and train() to add the new feature.

As you are only adding a new model, nothing that you do should change
your previous results.

----------------------------------------------------------------------

QUESTION 8.
Copy FileProb.java to SpeechRec.java.
Modify SpeechRec.java so that it does text categorization.
You only have one training corpus now, making things easier.

----------------------------------------------------------------------

QUESTION 10 [Extra Credit].

Modify BackoffWittenBellLanguageModel.java to complete this problem. To do so,
complete the implementation of the prob() method, and the implementation of the
train() method (this is overriding an implementation of LanguageModel.train()).
This should not change any of your previous results.

----------------------------------------------------------------------

NOTE FROM ELIAS PONVERT, WHO PORTED AN EARLIER VERSION OF THE CODE:

Some conceptual changes were made to this version from the Python version it was
based on. The big difference is the addition of an abstract class LanguageModel
which implements many of the less interesting methods, as well as a couple of
the more interesting methods, such as train(String trainFile). LanguageModel
specifies an abstract method prob(String x, String y, String z) which subclasses
must implement. This method calculates the trigram probability p(z | x,y )
according the the particular LM. Sample implementations are provided by
UnigramLanguageModel and AddLambdaLanguageModel, which are included in the
sample code. 

As mentioned, LanguageModel implements the train(String trainFile) method. This
method counts the token unigrams, bigrams and trigrams in the specified train
file -- these are stored in the map tokens. This training suffices for
UniformLanguageModel and AddLambdaLanguageModel, and others. This method is not
declared final, however, and may be overridden in subclasses.
